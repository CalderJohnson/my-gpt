**GPT plan and dev notes**

An implementation of the revolutionary transformer architecture for NLP and text generation.
Trainable on any text dataset, will generate new text in the style of that dataset.

Goal: Build a transformer decoder neural network that is trained on a corpus of text. It can then be fed an input, and it will generate what should come next (ie. a document completer)
